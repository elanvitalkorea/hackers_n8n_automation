
## 9-1 웹크롤링의 기본 
#### **웹 크롤링의 원리 🕸️**
웹 크롤링은 **로봇(Bot)** 또는 **스파이더(Spider)**라고 불리는 자동화된 프로그램이 웹사이트를 방문하여 정보를 수집하는 과정입니다. 사람이 브라우저를 통해 웹사이트에 접속하는 것과 유사하게, 크롤러는 웹 서버에 **HTTP 요청**을 보내 페이지의 **HTML 문서**를 받아옵니다. 그 후, 받아온 HTML 문서에서 원하는 데이터(예: 기사 제목, 가격, 날짜 등)를 추출하고 구조화된 형태로 저장합니다.

#### **웹사이트들이 웹 크롤링을 방어하는 방법 🛡️**
많은 웹사이트는 서버 부하, 데이터 무단 수집 등을 막기 위해 크롤링을 방어하는 기술을 사용합니다.
* **robots.txt**: 웹사이트 루트 디렉토리에 위치한 파일로, 어떤 페이지를 크롤링해도 되는지, 아닌지에 대한 규칙을 명시합니다. 크롤러는 이 규칙을 존중해야 합니다.
  * **예시 해석** (aitimes.com):
    * `User-agent: * / Disallow: /admin/`: 모든 검색엔진에 관리자 페이지만 차단
    * `User-agent: bingbot / Crawl-delay: 30`: Bing은 30초 간격으로 접근 제한
    * `Sitemap: https://www.aitimes.com/sitemap.xml`: 사이트 전체 구조 정보 제공
    * → 관리자 페이지만 차단하고 나머지는 모두 허용하는 **매우 관대한 정책** (뉴스 사이트의 일반적 패턴)
* **User-Agent 확인**: 서버는 요청을 보낸 클라이언트가 일반 브라우저인지, 아니면 봇인지 `User-Agent` 헤더를 통해 식별합니다. 봇으로 판단되면 접근을 차단할 수 있습니다.
* **IP 차단 및 Rate Limiting**: 짧은 시간에 비정상적으로 많은 요청을 보내는 IP 주소를 차단하거나 요청 횟수를 제한합니다.
* **CAPTCHA**: "나는 로봇이 아닙니다"와 같은 테스트를 통해 인간 사용자인지를 확인합니다.
* **동적 콘텐츠(JavaScript 렌더링)**: 초기 HTML 로딩 후, JavaScript를 통해 동적으로 콘텐츠를 불러오는 사이트입니다. 단순한 크롤러는 이 콘텐츠를 수집하지 못합니다.

#### **웹 크롤링 시 주의사항 및 대응 방안 ⚖️**
성공적이고 '착한' 크롤러를 만들기 위해 반드시 지켜야 할 몇 가지 규칙과 기술이 있습니다.

1.  **법적/윤리적 문제 확인 (가장 중요!)**
    * **`robots.txt` 규칙 준수**: 크롤링을 시작하기 전, 반드시 `https://사이트주소/robots.txt`를 방문하여 수집이 허용된 범위인지 확인하세요. 이는 웹 크롤링 세계의 가장 기본적인 예의입니다.
    * **이용 약관(Terms of Service) 확인**: 웹사이트의 이용 약관에 데이터 수집 및 사용에 대한 정책이 명시되어 있는 경우가 많습니다. 상업적 목적의 무단 크롤링을 금지하는 조항이 있는지 확인해야 합니다.
    * **개인정보 수집 금지**: 이름, 연락처, 이메일 등 개인을 식별할 수 있는 정보는 절대 수집해서는 안 됩니다.

2.  **서버에 부담을 주지 않기 (Rate Limit 대응)**
    * **문제점**: 자동화된 크롤러는 사람보다 훨씬 빠른 속도로 서버에 요청을 보냅니다. 이는 대상 서버에 DDOS 공격과 유사한 부담을 주며, **`429 Too Many Requests`** 오류와 함께 IP가 차단되는 주요 원인이 됩니다.
    * **대응 방안**: **'Polite Interval'**, 즉 **요청 사이에 충분한 시간 지연(Pause)**을 두어야 합니다. 마치 사람이 페이지를 둘러보는 것처럼, 각 요청 사이에 1~3초 정도의 간격을 두는 것이 일반적입니다.
    * **n8n 활용**: n8n에서는 **`Wait` 노드**를 루프(Loop) 안에 넣어 각 요청마다 일정한 지연 시간을 쉽게 설정할 수 있습니다.

3.  **크롤러 신분 위장하기 (차단 방지)**
    * **문제점**: 동일한 IP나 `User-Agent`의 반복적인 요청은 봇으로 쉽게 탐지되어 차단됩니다.
    * **대응 방안: IP 로테이션(Rotation)**
        * **프록시 서버(Proxy Server)**는 내 실제 IP를 숨기고 대신 요청을 보내주는 '인터넷 심부름꾼'입니다.
        * **n8n 적용법**: **`HTTP Request`** 노드의 **[Add Option] > [Proxy]** 메뉴에서 프록시 서비스 업체로부터 받은 `http://사용자이름:비밀번호@서버주소:포트` 형식의 주소를 입력하면, 해당 요청은 다른 IP로 전송되어 차단을 효과적으로 우회할 수 있습니다.
    * **대응 방안: User-Agent 랜덤화**
        * `HTTP Request` 노드의 헤더에 설정하는 `User-Agent` 값을 고정하지 않고, 여러 실제 브라우저의 `User-Agent` 값들을 리스트로 만들어 요청 시마다 무작위로 선택하여 보내면 더욱 효과적입니다.

요약하자면, 성공적인 크롤링은 기술적인 문제 해결뿐만 아니라, 대상 웹사이트를 존중하고 피해를 주지 않으려는 '착한 크롤러'의 자세에서 시작됩니다.

| **문제 상황** | **핵심 대응 방안** | **관련 n8n 노드/기능** |
| :--- | :--- | :--- |
| **서버 과부하 / Rate Limit** | 요청과 요청 사이에 의도적인 시간 지연 추가 | **`Wait` 노드** |
| **IP 주소 기반 차단** | 프록시 서버를 이용해 요청 IP를 계속 변경 | HTTP Request 노드의 `Proxy` 옵션 / Firecrawl 등 외부 서비스 |
| **User-Agent 기반 차단** | 실제 브라우저 정보로 위장하고, 여러 개를 무작위로 사용 | HTTP Request 노드의 `Headers` 옵션 |

#### **웹 크롤링의 난이도별 대응 방법**
* **[하] 정적 웹사이트**: HTML 구조가 단순하고 크롤링 방어 기술이 거의 없는 경우입니다. 간단한 `HTTP Request`만으로도 데이터를 쉽게 가져올 수 있습니다.
* **[중] User-Agent 확인 또는 로그인 요구**: `User-Agent` 헤더를 실제 브라우저처럼 설정하거나, 로그인 과정을 자동화하여 쿠키/세션 정보를 유지하는 방식으로 대응합니다.
* **[상] JavaScript 렌더링 및 안티봇 솔루션**: 페이지 콘텐츠가 JavaScript로 생성되거나 Cloudflare 같은 강력한 안티봇 기술이 적용된 경우입니다. 이때는 **Firecrawl**, Apify 같은 전문 API 서비스를 이용하거나 Puppeteer, Playwright 같은 헤드리스 브라우저(Headless Browser) 도구를 사용하는 것이 효과적입니다.

#### **헤드리스 브라우저 추가 설명**

JavaScript 실행 후 최종 결과가 필요한 경우(더보기 버튼, 무한 스크롤 등) 사용하는 **GUI 없는 브라우저**입니다. 실제 브라우저처럼 동작하며 클릭, 스크롤, 입력 등을 코드로 자동화할 수 있습니다.

##### **Puppeteer (Google/Chrome 최적화)**
**기본 흐름**: 브라우저 실행 → 페이지 이동 → 상호작용/대기 → 데이터 추출
- `page.goto()`: URL 이동
- `page.click()`: 요소 클릭
- `page.waitForSelector()`: 요소 로딩 대기
- `page.content()`: 최종 HTML 추출

##### **Playwright (Microsoft/크로스 브라우징)**
- **차별점**: Chrome, Firefox, Safari 모두 지원
- **자동 대기**: 요소가 준비될 때까지 자동으로 대기
- Puppeteer보다 안정적이고 코드가 간결함

##### **실전 선택 가이드**
**Puppeteer/Playwright의 한계**:
- 직접 코드 작성 및 유지보수 필요
- 높은 서버 리소스 소모
- 안티봇 탐지 가능성

**→ n8n 워크플로우에는 Firecrawl API 권장**
- 복잡한 과정을 API 한 번으로 처리
- 빠른 구축과 안정적 운영 가능
- 정밀한 제어가 필요한 특수한 경우에만 Puppeteer/Playwright를 별도 서버로 운영


## 9-2 DOM과 브라우저 개발자 도구 다루기

### **HTML/CSS DOM과 Selector 깊게 이해하기**

안녕하세요\! 이번 시간에는 웹 크롤링의 가장 핵심이 되는 개념인 **DOM**과 **CSS Selector**에 대해 조금 더 깊이 알아보겠습니다. 이 두 가지만 정확히 이해하면, 복잡해 보이는 어떤 웹사이트에서도 원하는 정보를 정확하게 '낚아챌' 수 있습니다.

#### **1. 우리 눈에 보이는 웹페이지의 진짜 모습: DOM 🏛️**

우리가 웹 브라우저로 보는 화면은 사실 예쁘게 렌더링된 결과물일 뿐, 그 본질은 **HTML**이라는 텍스트 문서입니다. 하지만 크롤러나 브라우저는 이 텍스트를 그대로 읽는 게 아니라, \*\*DOM(Document Object Model)\*\*이라는 구조로 변환해서 이해합니다.

**비유: 건축 설계도(HTML)와 실제 건물(DOM)**

  * **HTML**: 건물을 짓기 위한 **설계도**입니다. "여기에 기둥을 세우고, 2층에는 창문을 4개 만들어라" 같은 지침이 텍스트로 적혀있습니다.
  * **DOM**: 설계도를 바탕으로 실제로 지어진 \*\*'뼈대가 완성된 건물'\*\*입니다. 각 층(Parent), 각 방(Child), 그리고 같은 층의 옆방(Sibling)처럼 명확한 **계층 구조**를 가집니다.

브라우저는 HTML 설계도를 읽어서, 이 DOM이라는 구조화된 건물을 메모리상에 짓습니다. 우리는 이 건물 안을 돌아다니며 '2층의 세 번째 방에 있는 창문'처럼 원하는 요소를 찾아가는 것입니다.

**간단한 예시:**

이런 HTML 코드가 있다고 상상해 봅시다.

```html
<html>
<head>
    <title>뉴스 기사</title>
</head>
<body>
    <div id="main-content">
        <h1>오늘의 주요 뉴스</h1>
        <div class="article-list">
            <div class="item">
                <h2><a href="/news/1">첫 번째 기사 제목</a></h2>
                <p>기사 요약 내용...</p>
            </div>
            <div class="item">
                <h2><a href="/news/2">두 번째 기사 제목</a></h2>
                <p>다른 기사 요약...</p>
            </div>
        </div>
    </div>
</body>
</html>
```

브라우저는 이 코드를 아래와 같은 **DOM 트리(Tree)** 구조로 변환합니다.

```
html
├── head
│   └── title
└── body
    └── div (id="main-content")
        ├── h1
        └── div (class="article-list")
            ├── div (class="item")
            │   ├── h2
            │   │   └── a (href="/news/1")
            │   └── p
            └── div (class="item")
                ├── h2
                │   │   └── a (href="/news/2")
                └── p
```

이처럼 모든 요소는 **부모-자식 관계**로 연결됩니다. `body`는 `div#main-content`의 부모이고, 두 개의 `div.item`은 서로 **형제(Sibling)** 관계입니다. 크롤링은 바로 이 구조를 탐색하는 것입니다.

#### **2. DOM 건물의 주소 체계: CSS Selector 🗺️**

이제 '뼈대가 완성된 건물(DOM)'이 준비되었습니다. 여기서 "두 번째 기사의 제목만 가져와\!"라고 명령하려면 어떻게 해야 할까요? 바로 이때 사용하는 것이 **CSS Selector**, 즉 '주소'입니다.

CSS Selector는 원래 웹페이지를 꾸미기 위해 특정 요소에 스타일(색상, 크기 등)을 적용하려고 만들어졌지만, 크롤링에서는 **원하는 데이터를 찾는 주소**로 훨씬 더 중요하게 사용됩니다.

**주요 Selector 문법과 실전 예시:**

  * **태그(Tag) Selector**: 가장 기본입니다. 태그 이름을 그대로 사용합니다.

      * `h2`: 페이지의 모든 `<h2>` 태그를 선택합니다.
      * `p`: 페이지의 모든 `<p>` 태그(문단)를 선택합니다.
      * *단점*: 너무 광범위해서 단독으로 쓰이는 경우는 드뭅니다.

  * **ID Selector**: 고유한 이름표를 가진 단 하나의 요소를 선택합니다. `#` 기호를 사용합니다.

      * `#main-content`: 수많은 `div` 태그 중에서 `id`가 `main-content`인 **유일한 하나**를 찾아냅니다.
      * *장점*: 페이지에서 딱 하나만 존재하므로, 특정 영역을 지정할 때 가장 확실하고 빠릅니다.

  * **클래스(Class) Selector**: 같은 종류의 이름표를 가진 모든 요소를 선택합니다. `.` 기호를 사용합니다.

      * `.item`: `class`가 `item`인 모든 요소를 선택합니다. 위 예시에서는 두 개의 기사 `div`가 모두 선택됩니다.
      * *장점*: '기사 목록', '상품 목록'처럼 반복되는 요소들을 한 번에 그룹으로 선택할 때 매우 유용합니다.

  * **조합 Selector (가장 중요\!)**: Selector들을 조합하여 주소를 더욱 상세하게 만듭니다. 띄어쓰기는 '\~의 안에 있는' 이라는 뜻의 \*\*자손(Descendant)\*\*을 의미합니다.

      * **`.article-list .item`**

          * 해석: `class`가 `article-list`인 요소 **안에 있는** 모든 `.item` 요소들을 찾아줘.
          * 이렇게 범위를 좁혀나가면 다른 곳에 있는 `.item`은 무시하고, 기사 목록 안에 있는 것만 정확히 찾을 수 있습니다.

      * **`.item h2 a`**

          * 해석: `class`가 `item`인 요소 **안에 있는** `<h2>` 태그 **안에 있는** `<a>` 태그를 찾아줘.
          * 이 Selector를 사용하면 위 예시에서 "첫 번째 기사 제목"과 "두 번째 기사 제목"에 해당하는 `<a>` 태그 2개를 정확히 찾아낼 수 있습니다.


### **브라우저 개발자 도구 다루기**

브라우저 개발자 도구는 웹 크롤링의 필수 장비입니다. 원하는 데이터가 HTML 문서 어디에 있는지, 어떻게 가져올 수 있는지 분석할 수 있습니다.

#### **Element Selector 활용법**

1. 크롬 브라우저에서 크롤링할 페이지를 엽니다.
2. Inspector 도구 선택하여 모드를 바꿉니다. 
3.  추출하고 싶은 정보(예: 기사 제목) 위에서 마우스 오른쪽 버튼을 클릭하여 선택합니다.
4.  개발자 도구의 **Elements** 탭이 열리며 해당 요소의 HTML 코드가 하이라이트됩니다.
5.  하이라이트된 코드 위에서 다시 마우스 오른쪽 버튼을 클릭합니다.
      * **Copy \> Copy selector**: n8n의 HTML 노드에서 사용할 **CSS Selector 경로**를 복사합니다. (`#skin-9 > div:nth-child(3) > a > h2`)
      * **Copy \> Copy outerHTML**: 해당 요소의 전체 HTML 코드를 복사합니다.
6.  **요소 삭제 및 변경**: Elements 탭에서 특정 HTML 코드를 직접 삭제하거나 내용을 변경하며 웹페이지가 어떻게 바뀌는지 실시간으로 테스트해볼 수 있습니다. 이를 통해 크롤링에 방해되는 요소를 파악하고 제거하는 전략을 세울 수 있습니다.

#### **Network 탭 활용법**

서버와 브라우저가 어떤 통신을 하는지 엿볼 수 있는 강력한 기능입니다.

1.  개발자 도구를 열고 **Network** 탭으로 이동합니다.
2.  페이지를 새로고침(CMD+R / F5)하면 해당 페이지를 불러오기 위해 발생한 모든 네트워크 요청 목록이 나타납니다.
3.  목록에서 가장 위에 있는 문서(document) 파일을 클릭합니다.
4.  오른쪽에 나타나는 창에서 **Headers** 탭을 선택하고, **Request Headers** 섹션으로 스크롤합니다.
5.  `User-Agent` 항목을 찾아 값을 복사합니다. 이 값을 n8n의 HTTP Request 노드에 설정하면, 우리의 크롤러가 마치 실제 크롬 브라우저인 것처럼 위장하여 서버의 차단을 피할 확률을 높일 수 있습니다.


#### **HTML/CSS DOM과 Selector 깊게 이해하기 (실전 예제편)**

안녕하세요\! 웹 크롤링의 심장과도 같은 **DOM**과 **CSS Selector**에 대해 실제 AI TIMES 홈페이지 코드를 가지고 파헤쳐 보겠습니다. 이론으로만 보면 막막하지만, 실제 코드를 보면서 '주소'를 찾아가는 연습을 하면 금방 감을 잡으실 수 있습니다.

**오늘의 목표**: AI TIMES 홈페이지의 'Headlines' 섹션에서 **모든 기사의 제목과 기사로 연결되는 링크(URL)를 추출**하는 것입니다.

##### **1. 우리가 마주할 실제 설계도: HTML 코드 분석 🕵️‍♂️**

먼저, 개발자 도구(F12)로 'Headlines' 영역을 살펴보면 다음과 같은 `outerHTML` 구조를 가지고 있습니다. (실제로는 더 복잡하지만 핵심만 간추렸습니다.)

```html
<article class="box-skin idx--box " style="margin-top:50px">
    <header class="header"><strong>Headlines</strong></header>
    <section class="content">
        <div id="skin-8" class="auto-article">
            
            <div class="item">
                <div class="auto-content">
                    <a href="https://www.aitimes.com/news/articleView.html?idxno=203025" target="_top">
                        <h2 class="auto-titles line-x2 onload">엔비디아, 머스크의 '콜로서스 2' 지원 위해 200억달러 투자에 참여</h2>
                    </a>
                </div>
                <a href="https://www.aitimes.com/news/articleView.html?idxno=203025" target="_top" class="frame ratio-11 radius line">
                    <div class="auto-images thumb"> ... </div>
                </a>
            </div>

            <div class="item">
                <div class="auto-content">
                    <a href="https://www.aitimes.com/news/articleView.html?idxno=203029" target="_top">
                        <h2 class="auto-titles line-x2 onload">젠슨 황 "오픈AI에 지분 10% 넘긴 AMD 아이디어 기발해"&nbsp;</h2>
                    </a>
                </div>
                <a href="https://www.aitimes.com/news/articleView.html?idxno=203029" target="_top" class="frame ratio-11 radius line">
                    <div class="auto-images thumb"> ... </div>
                </a>
            </div>
            
            </div>
    </section>
</article>
```

여기서 보석 같은 정보들을 찾아봅시다.

  * **가장 큰 단서 (ID)**: 전체 기사 목록을 감싸는 `div`에 `id="skin-8"`이라는 아주 중요한 **고유 이름표**가 붙어있습니다. ID는 페이지 내에서 유일하기 때문에, 이 구역을 특정하는 완벽한 시작점입니다.
  * **반복되는 패턴 (Class)**: 각각의 기사는 `<div class="item">`이라는 동일한 **그룹 이름표**를 가지고 반복되고 있습니다.
  * **우리의 목표 데이터**:
      * **기사 제목**: `class="item"` 안에 `h2` 태그의 텍스트입니다.
      * **기사 링크**: `h2` 태그를 감싸고 있는 `a` 태그의 `href` 속성 값입니다.

##### **2. 최적의 경로 찾기: 더 좋은 CSS Selector 작성하기 🎯**

개발자 도구에서 'Copy Selector'를 하면 아래와 같이 매우 길고 복잡한 경로를 알려줄 때가 많습니다.

  * **브라우저가 알려준 경로 (나쁜 예시):**
    `#idx-default > div.index-grid-container > div > div > div.index-item > article`

이 주소는 "idx-default라는 ID를 가진 요소의 바로 밑에 있는 div 자식, 그 바로 밑에 있는 div..." 처럼 모든 경로를 하나하나 지정합니다. 이런 방식은 웹사이트 구조가 약간만 바뀌어도(예: 중간에 div 태그 하나 추가) 주소를 잃고 크롤링에 실패하게 되는 **매우 취약한(brittle) 방식**입니다.

우리는 방금 분석한 \*\*핵심 단서(ID, Class)\*\*를 이용해 훨씬 더 똑똑하고 강력한 주소를 만들 수 있습니다.

  * **우리가 만들 경로 (좋은 예시):**

    1.  **시작점 지정**: 먼저, 유일한 이름표인 `#skin-8`로 범위를 확실하게 좁힙니다.

        > `#skin-8`

    2.  **중간 경로 지정**: 그 안에서, 각 기사를 의미하는 `.item`을 찾습니다. 띄어쓰기는 '\~의 안에 있는' 이라는 뜻입니다.

        > `#skin-8 .item`

    3.  **최종 목표 지정**: 이제 우리의 최종 목표인 \*\*'제목'\*\*과 \*\*'링크'\*\*를 찾아봅시다.

          * **제목(Text)을 찾기 위한 최종 주소**: `.item` 안에 있는 `h2`
            > `#skin-8 .item h2`
          * **링크(URL)를 찾기 위한 최종 주소**: `.item` 안에 있는 `a`
            > `#skin-8 .item a`

이 Selector는 중간에 다른 태그가 추가되거나 구조가 일부 변경되어도 `#skin-8` 안에 있는 `.item` 속의 `h2`를 끈질기게 찾아내므로 훨씬 안정적입니다.

	  
	  
## 9-3 FireCrawl 기능 둘러보기
####  특징
![[Pasted image 20251009200242.png]]
- 단순히 웹페이지를 긁어오는 것을 넘어, **AI가 즉시 이해하고 활용할 수 있는 깨끗한 데이터로 변환해주는 API 서비스**
- 복잡한 HTML 구조, 광고, 불필요한 메뉴 등을 제거하고 본문 중심의 깔끔한 **마크다운(Markdown)** 형식으로 데이터를 정제
- 크롤링을 위한 자동 처리 능력
	- 프록시 및 IP 우회
	- 안티봇 메커니즘
	- Stealth Mode
	- 브라우저 환경 에뮬레이션 ( location 설정 등)

#### 가격
- free plan 500 credit 획득 가능
- Hobby 2만원대, Standard 12만원대 
- 전문적으로 크롤링 하는 곳은 standard 필요

#### 핵심 기능
##### **1. Scrape: 단일 URL에서 스마트하게 정보 추출하기**

가장 기본적이고 강력한 기능입니다. 특정 URL을 지정해 해당 페이지의 콘텐츠를 다양한 형태로 추출합니다.

  * **주요 기능**:
      * **다양한 포맷**: `markdown`, `html`, `links` 뿐만 아니라, 이제 `summary` 포맷을 통해 콘텐츠 요약본을 직접 받을 수 있습니다.
      * **구조화된 데이터 추출 (`JSON Mode`)**: 스키마(Schema)를 정의하거나, 간단한 **자연어 프롬프트**만으로 원하는 정보(예: 상품명, 가격, 작성자)를 정확하게 JSON 형태로 추출할 수 있습니다.
      * **페이지와 상호작용 (`Actions`)**: 스크레이핑 전에 **클릭, 스크롤, 텍스트 입력, 대기** 등의 사전 동작을 수행시켜 동적 콘텐츠나 로그인 후 페이지에 접근할 수 있습니다.
  * **n8n 응용 시나리오**:
      * **시장 동향 리서치**: 특정 기술 뉴스 URL을 입력받아 Firecrawl로 `summary`를 추출하고, 그 내용을 슬랙/디스코드로 매일 아침 전송.
      * **경쟁사 상품 모니터링**: 경쟁사 상품 페이지에서 `JSON Mode`를 사용해 가격과 재고 정보를 주기적으로 추출하고, 변동 시 구글 시트에 기록.

##### **2. Crawl: 웹사이트 전체를 지능적으로 탐색하기**

웹사이트 전체를 순회하며 모든 하위 페이지의 정보를 비동기적으로 수집합니다.

  * **주요 기능**:
      * **프롬프트 기반 크롤링**: **"블로그 섹션만 크롤링해줘"** 와 같은 자연어 프롬프트로 복잡한 크롤링 규칙(포함/제외 경로)을 자동으로 생성할 수 있습니다.
      * **비동기 처리 및 Webhook 지원**: 크롤링 요청 시 작업 ID를 즉시 반환하며, 작업이 진행되는 동안 **Webhook**을 통해 각 페이지가 수집될 때마다 n8n으로 실시간 데이터를 전송할 수 있습니다. 이는 폴링(Polling) 방식보다 훨씬 효율적입니다.
  * **n8n 응용 시나리오**:
      * **실시간 RAG 데이터베이스 구축**: 회사 기술 블로그에 새 글이 올라오면, 크롤링 후 Webhook으로 처리된 페이지 내용을 받아 즉시 벡터 데이터베이스(Supabase, Pinecone)에 저장하여 AI 챗봇의 지식을 항상 최신으로 유지.
      * **문서/가이드 자동 백업**: 특정 서비스의 온라인 매뉴얼을 매주 크롤링하여 변경된 페이지만을 노션(Notion)에 업데이트.

##### **3. Search: 검색어로 웹 정보를 수집하고 바로 추출하기**

Firecrawl 내장 검색 엔진으로 키워드 검색과 결과 페이지 스크레이핑을 한 번에 수행합니다.

  * **주요 기능**:
      * **다중 소스 검색**: 일반 웹(`web`) 뿐만 아니라 \*\*뉴스(`news`), 이미지(`images`)\*\*까지 검색 범위를 확장할 수 있습니다.
      * **전문 분야 검색**: `categories` 파라미터를 사용해 **GitHub**나 \*\*학술자료(Research)\*\*로 검색 범위를 좁힐 수 있어 전문적인 리서치에 유용합니다.
  * Firecrawl Playground
	  * `n8n jobs in Sanfrancisco`
	  * `https://www.firecrawl.dev/playground`
  * **n8n 응용 시나리오**:
      * **자동화된 콘텐츠 큐레이션**: 'AI Agent' 키워드로 `news` 소스를 검색해 최신 뉴스 기사들을 스크랩하고, LLM으로 요약하여 주간 뉴스레터 초안을 자동으로 생성.

##### **4. Map & Extract: URL 탐색과 대규모 데이터 추출**

2.0에 추가된 강력한 기능들입니다.

  * **`Map`**: 크롤링보다 훨씬 **빠르게** 웹사이트의 전체 URL 구조(사이트맵)를 파악할 때 사용합니다. 특정 페이지들을 스크랩하기 전에 먼저 전체 구조를 파악하는 데 유용합니다.
  * **`Extract`**: 여러 URL 또는 \*\*와일드카드(`elanvital.ai/blog/*`)\*\*를 사용하여 특정 도메인의 모든 관련 페이지에서 구조화된 데이터를 한 번에 대규모로 추출하는 데 특화된 기능입니다. AI 에이전트 **FIRE-1**을 활용한 복잡한 추출 작업도 지원합니다.
  * **n8n 응용 시나리오**:
      * **리드 발굴 자동화**: 특정 산업군 기업 목록을 확보한 뒤, 각 기업 웹사이트의 채용 페이지(`company.com/careers/*`)를 `Extract` 기능으로 분석해 특정 직무 채용 공고를 실시간으로 수집.

##### **5. 작업 완료 시 웹훅 지원**

이제 폴링(일정 시간마다 상태를 확인하는 방식)이 필요 없습니다\! 
Firecrawl의 Webhook을 사용하면 작업이 완료되거나 데이터가 준비될 때마다 Firecrawl이 n8n을 직접 호출하게 할 수 있습니다.

1.  n8n에서 **Webhook 노드**를 추가하고 테스트 URL을 복사합니다.
2.  Firecrawl의 `crawl` 또는 `batchScrape` API를 호출할 때, Body에 `webhook` 객체를 추가합니다.

```json
{
  "url": "https://elanvital.ai/blog",
  "webhook": {
    "url": "YOUR_N8N_WEBHOOK_URL",
    "events": ["page", "completed"]
  }
}
```


#### REST CLIENT API 둘러보기

##### 1. Scrape 작업 - 프롬프트 Only 

- 📝 설명
AI타임스 기사에서 제목, 본문 내용, 기자 이름을 한국어 프롬프트로 추출하는 요청입니다.

- 🔗 curl 명령어
```bash
curl -X POST https://api.firecrawl.dev/v2/scrape \
  -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://www.aitimes.com/news/articleView.html?idxno=202947",
    "formats": [
      {
        "type": "json",
        "prompt": "이 기사에서 제목(title), 전체 본문 내용(content), 그리고 기자 이름(author)을 추출해서 알려줘."
      }
    ]
  }'
```

##### 2. Scrape 작업 - 스키마 + 서머리

- 📝 설명
JSON 스키마를 사용한 구조화된 데이터 추출과 함께 요약(summary)을 함께 받는 요청입니다.

- 🔗 curl 명령어
```bash
curl -X POST https://api.firecrawl.dev/v2/scrape \
  -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://www.aitimes.com/news/articleView.html?idxno=202946",
    "formats": [
      "summary",
      {
        "type": "json",
        "schema": {
          "type": "object",
          "properties": {
            "title": {
              "type": "string",
              "description": "기사 제목"
            },
            "contents": {
              "type": "string",
              "description": "본문 전체 텍스트(HTML 태그 제거)"
            },
            "author": {
              "type": "string",
              "description": "저자 또는 기자 이름"
            },
            "timestamp": {
              "type": "string",
              "description": "발행 시간 (yyyy-MM-dd HH:mm)"
            }
          },
          "required": ["title", "contents", "author", "timestamp"]
        }
      }
    ]
  }'
```

##### 3. Scrape 작업 - YouTube 동영상

- 📝 설명
YouTube 동영상의 자막이나 내용을 Markdown 형식으로 추출하는 요청입니다.

- 🔗 curl 명령어
```bash
curl -X POST https://api.firecrawl.dev/v2/scrape \
  -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://www.youtube.com/watch?v=idccdssOZQ8",
    "formats": ["markdown"]
  }'
```

##### 4. Extract 작업 - 비동기 추출 시작

- 📝 설명
여러 URL에서 데이터를 비동기적으로 추출하는 작업을 시작합니다. 작업 ID를 반환받아 결과를 조회할 수 있습니다.

- 🔗 curl 명령어
```bash
curl -X POST https://api.firecrawl.dev/v2/extract \
  -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "urls": ["https://www.aitimes.com/news/articleView.html?idxno=202947"],
    "prompt": "Extract the title, content, and the author from this news article."
  }'
```


##### 5. Extract 결과 조회

- 📝 설명
Extract 작업의 결과를 조회합니다. 이전 요청에서 받은 작업 ID를 사용합니다.

- 🔗 curl 명령어
```bash
# 작업 ID를 변수로 설정
EXTRACT_ID="extract_1234567890abcdef"

curl -X GET "https://api.firecrawl.dev/v2/extract/$EXTRACT_ID" \
  -H "Authorization: Bearer $FIRECRAWL_API_KEY"
```

##### 6. Search 작업 - AI 에이전트 검색

- 📝 설명
"AI agents" 키워드로 뉴스 소스에서 검색하고, Markdown과 링크 형식으로 결과를 받습니다.

- 🔗 curl 명령어
```bash
curl -X POST https://api.firecrawl.dev/v2/search \
  -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "ai agents",
    "sources": ["news"],
    "limit": 5,
    "scrapeOptions": {
      "formats": ["markdown", "links"]
    }
  }'
```

##### 7. Map 작업 - 웹사이트 맵 생성

- 📝 설명
주어진 URL의 전체 사이트 맵을 생성합니다. CrewAI 문서 사이트를 예시로 사용합니다.

- 🔗 curl 명령어
```bash
curl -X POST https://api.firecrawl.dev/v2/map \
  -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://docs.crewai.com/"
  }'
```


#### API 검색 Project 구축
- Gemini Gem 을 활용 (ChatGPT Project 등 무방)
- 대상 https://docs.firecrawl.dev 
- npx sitefetch 통해 md 파일 하나로 가져옴
	- `npx sitefetch https://docs.firecrawl.dev -o firecrawl-doc.md`
- 해당 파일을 Gem의 지식에 추가


## 9-4 웹 크롤링 워크플로우 완성

이번 실습에서는 두 가지 시나리오의 워크플로우를 완성합니다.

1.  **MOST POPULAR 크롤링**: AI TIMES 홈페이지의 'Most Popular' 섹션 기사 **하나**를 가져와 Firecrawl로 본문을 추출하고, Gemini로 요약하여 Discord에 게시합니다.
2.  **HEADLINE 크롤링**: 'Headlines' 섹션의 **모든** 기사를 크롤링하여 본문을 하나로 합친 뒤, Gemini로 전체 내용을 요약하여 Gmail로 발송합니다.

#### **시나리오 1: MOST POPULAR 기사 요약 및 Discord 게시**

이 워크플로우는 수동으로 실행되며, 다음과 같은 순서로 동작합니다.

1.  **`Get AITimes Homepage` (HTTP Request 노드)**

      * **목적**: AI TIMES 홈페이지(`https://www.aitimes.com/`)의 전체 HTML 소스를 가져옵니다.
      * **핵심 설정**: Request Headers의 \*\*`User-Agent`\*\*를 실제 크롬 브라우저 값으로 설정하여 크롤러가 아닌 일반 사용자인 것처럼 위장합니다.

    ```json
    {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.5615.136 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    }
    ```

2.  **`Scrape Most Popular Section` (HTML 노드)**

      * **목적**: 이전 노드에서 받은 HTML 소스에서 'Most Popular' 섹션의 기사 목록(순위, 제목, 링크)을 추출합니다.
      * **핵심 설정**: 개발자 도구로 찾은 CSS Selector를 사용하여 원하는 데이터를 정확히 지정합니다.
          * **`rank`**: `#skin-9 .item .number`
          * **`title`**: `#skin-9 .item a h2`
          * **`link`**: `#skin-9 .item a` (추출 값: `href` Attribute)
      * 이 노드를 거치면 rank, title, link가 각각의 배열 형태로 출력됩니다.

3.  **`Make target objects` (Code 노드)**

      * **목적**: 분리된 3개의 배열(rank, title, link)을 하나의 배열로 합치고, 각 기사가 하나의 객체(Object)가 되도록 데이터를 재구성합니다. 이는 이후 노드에서 각 기사를 개별적으로 다루기 쉽게 만듭니다.
      * **핵심 코드**: `map()` 함수를 사용하여 각 배열의 동일한 인덱스에 있는 값들을 묶어 새로운 객체를 만듭니다.


    ```javascript
    const { rank, title, link } = items[0].json;

    const results = rank.map((r, index) => ({
      json: {
        rank: r,
        title: title[index],
        link: link[index]
      }
    }));

    return results;
    ```

4.  **`Limit` (Limit 노드)**

      * **목적**: 테스트를 위해 전체 워크플로우를 실행할 아이템의 수를 제한합니다. 예를 들어 '1'로 설정하면 가장 인기 있는 기사 하나에 대해서만 자동화가 실행됩니다.

5.  **`firecrawl only prompt` (HTTP Request 노드)**

      * **목적**: 개별 기사의 링크를 받아 **Firecrawl API**를 호출하여 기사의 핵심 내용(제목, 본문, 기자 이름)을 AI를 통해 지능적으로 추출합니다.
      * **핵심 설정**:
          * **URL**: `https://api.firecrawl.dev/v2/scrape`
          * **Authentication**: `Header Auth`를 사용하며, 미리 발급받은 Firecrawl API 키를 인증 정보로 사용합니다.
          * **JSON Body**: Firecrawl에 요청할 내용을 담습니다. `url` 값으로는 이전 노드에서 넘어온 기사 링크 `{{ $json.link }}`를 동적으로 넣어줍니다. `prompt`를 사용해 원하는 정보를 자연어로 지시합니다.
        <!-- end list -->
        ```json
        {
          "url": "{{ $json.link }}",
          "formats": [
            {
              "type": "json",
              "prompt": "이 기사에서 제목(title), 전체 본문 내용(content), 그리고 기자 이름(author)을 추출해서 알려줘."
            }
          ]
        }
        ```

6.  **`Summary Article` (Google Gemini 노드)**

      * **목적**: Firecrawl이 추출한 기사 본문(`content`)을 Google Gemini 모델을 이용해 5줄의 불릿포인트로 요약합니다.
      * **핵심 설정 (Prompt)**:


    ```
    하기 AI 뉴스 기사를 5줄 불릿포인트 및 개조식으로 요약해주세요. 요약에 대한 정보만 주세요.
    >>>
    {{ $json.data.json.content }}
    ```

7.  **`Publish to Discord` (Discord 노드)**

      * **목적**: 최종적으로 요약된 기사 내용을 Discord 채널에 보기 좋은 **임베드(Embed)** 형식으로 게시합니다.
      * **핵심 설정**: 임베드의 각 필드(제목, 설명, URL 등)에 이전 노드들에서 가져온 데이터를 n8n 표현식(`{{ }}`)을 사용하여 동적으로 채워 넣습니다.
          * **Title**: `{{ $('firecrawl only prompt').item.json.data.json.title }}`
          * **URL**: `{{ $('firecrawl only prompt').item.json.data.metadata.url }}`
          * **Description**: `{{ $json.content.parts[0].text }}` (Gemini 요약 결과)
          * **Author**: `{{ $('firecrawl only prompt').item.json.data.json.author }}`

-----

#### **시나리오 2: HEADLINE 기사 전체 요약 및 Gmail 발송**

이 워크플로우는 매일 오전 10시에 자동으로 실행되도록 **Schedule Trigger**로 시작됩니다.

1.  **`Get AITimes Homepage1` → `Headline sector extrator` → `Make target objects1`**: 시나리오 1과 동일한 원리로 동작하지만, 'Headlines' 섹션(`id="skin-8"`)에서 기사 제목과 링크 목록을 가져옵니다.

2.  **`Loop Over Items` (SplitInBatches 노드)**

      * **목적**: 여러 개의 헤드라인 기사를 하나씩 순회하며 처리하기 위한 \*\*루프(Loop)\*\*를 생성합니다.

3.  **`HTTP Request` → `Markdown` → `Wait` (루프 내부)**

      * 루프는 각 기사 링크에 대해 HTTP 요청을 보내 전체 HTML을 가져온 뒤, `Markdown` 노드를 통해 텍스트만 추출합니다. `Wait` 노드는 서버에 과도한 부하를 주지 않기 위해 각 요청 사이에 1초의 지연 시간을 둡니다.

4.  **`Aggregate` (Aggregate 노드)**

      * **목적**: 루프를 돌며 개별적으로 추출된 모든 기사의 본문 텍스트를 하나의 데이터로 합칩니다.

5.  **`Summary Whole Article` (Google Gemini 노드)**

      * **목적**: 합쳐진 전체 기사 본문을 Gemini에 전달하여 HTML 형식의 요약문을 생성하도록 요청합니다.
      * **핵심 설정 (Prompt)**: 각 기사별로 제목, 요약 3줄, 링크를 포함하고, 제목은 볼드 처리하는 등 Gmail 본문에 적합한 형태로 결과물을 만들도록 구체적으로 지시합니다.


    ```
    AITIMES 뉴스기사를 요약해줘. 
    - 각 기사별 제목, 요약 3줄, 링크로 구성해주세요
    - Gmail로 보내는 본문으로 HTML 지원됨
    - 타이틀은 볼드 처리해주세요 
    - 요약은 불릿 포인트 3줄이고 본문 이외에 불필요요소 모두 제거
    {{ $json.data }}
    ```

6.  **`Send a message` (Gmail 노드)**

      * **목적**: Gemini가 생성한 HTML 요약문을 지정된 이메일 주소로 발송합니다. 메일 제목에는 `{{ $now.format('yyyy-MM-dd') }}` 표현식을 사용해 오늘 날짜를 동적으로 포함시킵니다.

